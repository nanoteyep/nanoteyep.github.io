---
layout: post
title:  "Road to Datascientist - 25. LDA"
date:   2021-08-24
use_math: true
comments: true
categories: DataScience python ML
---
# Linear Discriminant Analysis

---

# 1. LDH란

![ldh_1](/img/lda_1.png)

> 먼저 LDH는 두가지 가정이 존재합니다.

> 첫번째, **각 집단은 정규분포 형태의 확률분포를 가진다.**

> 두번째, **각 집단의 공분산은 서로 비슷한 형태의 구조를 가진다.**

> 이러한 가정하에 ***Decision boundary***를 생성하여 분류(classification)하는 모델입니다.

* **Decision boundary**

![lda_2](/img/lda_2.png)

> decision boundary는 평균의 차를 최대화 하는 방향으로 설정 해야 확실하게 두 그룹을 구분 할 수 있습니다.

> 그렇지만 첫번째 그림처럼 무작정 평균의 차를 최대화 시킨다면 두 그룹의 분산이 너무 크기 때문에 제대로 된 분류를 할 수 없을 가능성이 큽니다.

> 그렇기 때문에 LDA의 decision boundary는 오른쪽 그림과 같이 분산대비 평균의 차이가 극대화 하는 boundary를 찾아야 합니다.

![lda_3](/img/lda_3.png)

> LDH 에서 decision boundary는 위와 같은 식을 가지고 있습니다.

> 이 식을 이해하기 위해서는 아래 설명할 두가지 수학적 개념을 이해하여야 합니다.

---

# 2. 다변량 정규분포

* **정규분포**

![lda_4](/img/lda_4.png)

* **이변량 정규분포**

![lda_5](/img/lda_5.png)

* **다변량 정규분포**

![lda_6](/img/lda_6.png)

> p는 그룹의 갯수입니다, 또한 시그마는 공분산 행렬입니다.

> LDH에는 두개 이상의 그룹이 존재할테지만 한개의 decision boundary를 그리기 위해서는 우선 두개의 그룹을 뽑아 순차적으로 비교해야 합니다.

> 이때 선발된 그룹을 k,l 그룹이라고 칭하겠습니다.

![lda_8](/img/lda_8.png)

> 우선 p개의 그룹에서 k그룹의 변수가 나올 확률을 $\pi k$ , 사전확률 이라고 합니다. 그렇다면 다변량 정규분포에서 k 그룹이 나올 확률은 확률분포함수에 $\pi$를 곱한 값이 될 것 입니다.

> 그렇다면 이러한 확률식을 간단히 계산하기 위해 사전확률을 고려해서 로그를 취하고 k그룹과 l그룹을 비교하면 다음과 같이 식이 형성됩니다.

![lda_9](/img/lda_9.png)

> 이때 평균을 제외한 나머지 term은 전부 스칼라 값 입니다. 따라서 k, l 그룹의 확률분포함수는 각 그룹의 평균벡터와 같은 방향을 가지며 이를 서로 빼주었을 때 (수정중-------------------------------------------------------------------------------------------------------)

> LDH에서는 그룹들의 공분산 구조가 서로 비슷하다고 가정합니다. 그렇기에 $\sum k$ 와 $\sum l$은 서로 같다고 가정하는 것과 같습니다.

> 이 때 두 그룹의 확률분포식을 빼 준 것이 바로 LDH의 decision boundary가 됩니다.

![lda_10](/img/lda_10.png)

---

# 3. Projection(사영)

![lda_11](/img/lda_11.png)

> Projection의 정의는 위와 같습니다.

> 특정 벡터를 특정 축에 수직선을 내려 그 크기와 방향을 나타내는 것이며 만약 u가 크기가 1인 방향벡터라면 projection의 값은 두 벡터의 내적과 같습니다.


